---
output:
  pdf_document: default
  html_document: default
---
```{r reprex-options, include = FALSE}
options(
  keep.source = TRUE,
  rlang_backtrace_on_error_report = "full",
  crayon.enabled = FALSE,
  reprex.current_venue = "gh"
)
```

```{r, results = 'asis', echo = FALSE, include = file.exists('.Rprofile'), eval = file.exists('.Rprofile')}
cat(sprintf("*Local `.Rprofile` detected at `%s`*", normalizePath(".Rprofile")))
```

---
output:
  reprex::reprex_document:
    venue: "gh"
    advertise: FALSE
    session_info: TRUE
    style: TRUE
    comment: "#;-)"
    tidyverse_quiet: FALSE
    std_out_err: TRUE
knit: reprex::reprex_render
---
# Encoding Categorical Predictors
Agazzi Ruben <br />
Davide Dell'Orto  <br />
Hellem Carrasco  <br />

## Abstract

This is a comprehensive analysis of how categorical, and more general non numerical data can be encoded into a numerical form, in order to be used by all types of models.
In particular will be addressed how to manage unordered and ordered categorical data, and also how to extract features from textual data.

## Summary

## Chapter 1

Handling categorial data is a very important step during the implementation of a statistical or machine learning model. Most of the models does not accept categorical data, but only numerical data. The only models that accepts this type of data are tree-based model which can handle this type of data by default.

### Dummy Variables

The most basic approach used for handling unordered categorical data, consists in creating dummy or indicator variables. The most common form is creating binary dummy variables for categorical predictors: if the categorical predictor can assume 3 values we can create 2 binary dummy variables where value 1 will be, for example, d1=1 and d2=0, value 2 will be d1=0 and d2=1 and where value 3 will be d1=0 and d2=0. We could also represent this type of categorical data with 3 dummy variables, but this approach could lead, sometimes, to problems: some models, in order to estimate their parameters, needs to invert the matrix $(X'X)$, if the model has an intercept an additional column of one for all columns is added to the $X$ matrix; if we add the new 3 dummy variables this will add to the previous intercept row, and this linear combination would prevent the $(X'X)$ matrix to be invertible. This is the reason why we encode $C$ different categories into $C-1$ dummy variables

### Code

This piece of code shows a way of encoding the seven days of weeks into 6 dummy variables
```{r}
library(tidymodels)
library(FeatureHashing)
library(stringr)
library('fastDummies')
days_of_week <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday","Friday", "Saturday")
test_df <- data.frame(days_of_week)
test_df <- dummy_cols(test_df, select_columns = c('days_of_week'))
test_df = test_df[-c(5)]
```
![Table of encoded days of week using 6 dummy variables](./images/dummy_days_of_week_2.png)

## Encoding predictors mith many categories
We may have a problem by encoding values using simple dummy variables: if the categorical predictor can assume a large number of different values, like for example the ZIP code, we will end up with too many dummy variables.
If we make a resampling on this type of data there is also the problem where some dummy variables will assume an always 0 value: this case is called _zero-variance predictor_. Using the example of ZIP codes, highli popolated areas will lead to higher frequencies of the same ZIP codes, while some will be very rare, in this case the dummy variables could become _near-zero-variance predictors_. In this cases we could remove _zero-variance and near-zero-variance predictors_.

### Hashing functions
Instead of removing the mentioned types of predictors we could use an hashing function to map the values into fewer values, and then use these values to create dummy variables. the process of using hashes to create dummy variables is called _feature hasing_ or _hash trick_.
The process of hashing features is:

#. Calculate the hash from the feature value, which is an integer value.
#. Calculate the new feature value, if we want 16 different values we calculate _(hashValue mod 16) +1_

Some considerations needs to be done: because of the nature of the hashing functions, sometimes we will have collisions. Collisions occurs when two different values has different hashing. In statistics this is an aliasing problem. A way to solve this problem consists into using signed hashes, so instead of only 0 and 1 values for dummy variables, we can also have -1 value in order to reduce aliasing.

### Problems of feature hashing

The process of feature hashing can lead to some problems:

#. Collisions: Even with signed hashes collisions can appear, and in this cases is more difficult to understand the impact of a category on the model because of this aliasing problem.
#. Collision meaning: if two categories collides, this does not mean that they share something in common or have some type of correlation or similarity.
#. Collision probability: as hashing function does not have information about the distribution of data, rarer categories can collide with more high frequency categories, in this case the higher frequency category will have much more influence.

### Code 
This piece of code shows how feature hashing can be done. The dataset used is the OkCupid dataset and the categorical predictor used was the _where_town_ column of the dataset.
```{r}
library(tidymodels)
library(FeatureHashing)
library(stringr)

options(width = 150)


load("./Datasets/okc.RData")

towns_to_sample <- c(
  'alameda', 'belmont', 'benicia', 'berkeley', 'castro_valley', 'daly_city', 
  'emeryville', 'fairfax', 'martinez', 'menlo_park', 'mountain_view', 'oakland', 
  'other', 'palo_alto', 'san_francisco', 'san_leandro', 'san_mateo', 
  'san_rafael', 'south_san_francisco', 'walnut_creek'
)

# Sampled locations from "where_town" column
locations_sampled <- okc_train %>% dplyr::select(where_town) %>% distinct(where_town) %>% arrange(where_town)

hashes <- hashed.model.matrix(
  ~ where_town,
  data = locations_sampled,
  hash.size = 2^4,
  signed.hash=FALSE,
  create.mapping=TRUE
)

hash_mapping = hash.mapping(hashes)
names(hash_mapping) = str_remove(names(hash_mapping), 'where_town')

# Takes hash mapping, converts to dataframe, set new columns names, calculate hash over name to have original integer value, filter for selected cities 
binary_calcs = hash_mapping %>% enframe() %>% set_names(c('town', 'column_num_16')) %>% mutate(integer_16 = hashed.value(names(hash_mapping))) %>% dplyr::filter(town %in% towns_to_sample) %>% arrange(town)

hashes_df = hashes %>% 
  as.matrix() %>%
  as_tibble() %>%
  bind_cols(locations_sampled) %>%
  dplyr::rename(town = where_town) %>% 
  dplyr::filter(town %in% towns_to_sample) %>% 
  arrange(town)


# Making a signed hasing version in order to prevent aliasing

hashes_signed <- hashed.model.matrix(
  ~ where_town,
  data = locations_sampled,
  hash.size = 2^4,
  signed.hash=TRUE,
  create.mapping=TRUE
)
hashes_df_signed = hashes_signed %>% 
  as.matrix() %>%
  as_tibble() %>%
  bind_cols(locations_sampled) %>%
  dplyr::rename(town = where_town) %>% 
  dplyr::filter(town %in% towns_to_sample) %>% 
  arrange(town)


```

## Features from text data

```{r}
load("./Datasets/okc.RData")

stem_data = okc_sampled[okc_sampled$Class == "stem",]
non_stem_data = okc_sampled[okc_sampled$Class != "stem",]
result_stem = grepl("http|https", stem_data$essays)
result_non_stem = grepl("http|https", non_stem_data$essays)

number_stem_with_link = sum(result_stem)
number_non_stem_with_link = sum(result_non_stem)

prop.test(x=number_stem_with_link, n = 10000, alternative='two.sided')

odds_ratio = (number_non_stem_with_link * (length(result_non_stem) - number_non_stem_with_link))/((length(result_stem) - number_stem_with_link) * number_non_stem_with_link)


```

```{r}
library(caret)
library(tidymodels)
library(keras)
library(doParallel)
library(pROC)

load("./Datasets/okc.RData")
load("./Datasets/okc_other.RData")
load("./Datasets/okc_binary.RData")
load("./Datasets/okc_features.RData")

# joining all pre-computed features data sets: joins basic dataset, binary dataset and basic precomputed features.
okc_train = okc_train %>% full_join(okc_train_binary) %>%full_join(basic_features_train) %>% arrange(profile) %>% dplyr::select(-profile)

# Selecting pre-computed textual features names.
text_features = c("n_urls", "n_hashtags", "n_mentions", "n_commas", "n_digits",
    "n_exclaims", "n_extraspaces", "n_lowers", "n_lowersp", "n_periods",
    "n_words", "n_puncts", "n_charsperword")

#Specifying sentence features names.
sentiment_features = c("sent_afinn", "sent_bing", "n_polite", "n_first_person", "n_first_personp",
    "n_second_person", "n_second_personp", "n_third_person", "n_prepositions")

# Getting data set base features names.
base_features = names(okc_test)
base_features = base_features[!(base_features %in% c('Class','profile','essay_length','where_state'))]

#Getting pre-computed keyword features names.
keyword_features = names(okc_train_binary)
keyword_features = keyword_features[keyword_features != 'profile']

#Function used to get useful statistics from data and specified model
statistiche = function(data, lev= levels(data$obs), model = NULL){
  c(
    twoClassSummary(data = data, lev = levels(data$obs), model),
    prSummary(data = data, lev = levels(data$obs), model),
    mnLogLoss(data = data, lev = levels(data$obs), model),
    defaultSummary(data = data, lev = levels(data$obs), model)
  )
}

# used to get computational data of the models
okc_train_control = trainControl(
  method = "cv",
  classProbs = TRUE,
  summaryFunction = defaultSummary,
  returnData = FALSE,
  trim = TRUE,
  sampling = "down"
)

# keyword normalization
keyword_normalization = 
  #transforms dataset to recipe object with basic and keyword features
  recipe(Class ~.,data=okc_train[, c("Class", base_features, keyword_features)]) %>% 
  step_YeoJohnson(all_numeric()) %>%
  step_other(where_town) %>%
  # Creates dummy variables when needed
  step_dummy(all_nominal(), -all_outcomes()) %>%
  # Remove zero variance predictors
  step_zv(all_predictors()) %>%
  #Centers all predictors
  step_center(all_predictors()) %>%
  # Scale all predictors to have a std dev equal to 1
  step_scale(all_predictors())

okc_control_rand = okc_train_control
okc_control_rand$search = "random"

set.seed(49)

#multi_layer_perceptron_keyword = train(
#  keyword_normalization,
#  data = okc_train,
#  method = "mlpKerasDropout",
#  metric= "ROC",
#  tuneLength = 20,
#  trControl=okc_control_rand,
#  verbose = 0,
#  epochs = 500
#)



#Model with basic profile info

basic_model =
  recipe(Class ~ ., data=okc_train[, c('Class', base_features)]) %>% 
  step_YeoJohnson(all_numeric()) %>%
  step_other(where_town) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors())

set.seed(49)

glm_basic <- train(
  basic_model,
  data = okc_train,
  method = "glm",
  metric = "ROC",
  trControl = okc_ctrl
)
basic_pred <- ncol(glm_basic$recipe$template) - 1

#Model with basic text features
text_rec <-
  recipe(Class ~ .,
         data = okc_train[, c("Class", base_features, text_features)]) %>%
  step_YeoJohnson(all_numeric()) %>%
  step_other(where_town) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors())

set.seed(49)
glm_text <- train(
  text_rec,
  data = okc_train,
  method = "glm",
  metric = "ROC",
  trControl = okc_ctrl
)

text_pred <- ncol(glm_text$recipe$template) - 1

#Model with base info, text features and keywords
keyword_rec <-
  recipe(Class ~ .,
         data = okc_train[, c("Class", base_features, text_features, keyword_features)]) %>%
  step_YeoJohnson(all_numeric()) %>%
  step_other(where_town) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors())

okc_ctrl_keep <- okc_ctrl
okc_ctrl_keep$savePredictions <- "final"

set.seed(49)
glm_keyword <- train(
  keyword_rec,
  data = okc_train,
  method = "glm",
  metric = "ROC",
  trControl = okc_ctrl_keep
)

keyword_pred <- ncol(glm_keyword$recipe$template) - 1

#Model with base info, text features, keywords and sentiment analysis

sent_rec <-
  recipe(Class ~ .,
         data = okc_train[, c("Class", base_features, keyword_features, sentiment_features)]) %>%
  step_YeoJohnson(all_numeric()) %>%
  step_other(where_town) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors())

set.seed(49)
glm_sent <- train(
  sent_rec,
  data = okc_train,
  method = "glm",
  metric = "ROC",
  trControl = okc_ctrl
)

sent_pred <- ncol(glm_sent$recipe$template) - 1

#Data collection and visualization
features_groups <-
  c("Basic Profile Info", "+ Simple Text Features", "+ Keywords", "+ Keywords\n+ Sentiment")

glm_resamples <-
  glm_basic$resample %>%
  mutate(Features = "Basic Profile Info") %>%
  bind_rows(
    glm_text$resample %>%
      mutate(Features = "+ Simple Text Features"),
    glm_keyword$resample %>%
      mutate(Features = "+ Keywords"),
    glm_sent$resample %>%
      mutate(Features = "+ Keywords\n+ Sentiment")
  ) %>%
  mutate(Features = factor(Features, levels = features_groups))

glm_resamples_mean <-
  glm_resamples %>%
  group_by(Features) %>%
  summarize(ROC = mean(ROC))
glm_resamples_mean$Resample = 'Average'

temp_df = glm_resamples[,-c(2,3)]

full_df = rbind(glm_resamples_mean,temp_df )
library(ggplot2)


ggplot(data = subset(full_df, Resample != 'Average'),aes(x=Features, y=ROC, colour=Resample, group=Resample))+ geom_line() + geom_point() + geom_point(data = subset(full_df,  Resample == 'Average'),aes(x=Features, y=ROC,),color = 'black')+geom_line(data = subset(full_df,  Resample == 'Average'),size=1,aes(x=Features, y=ROC,),color = 'black')

```

```{r}
library(caret)
library(tidymodels)
library(keras)
library(doParallel)

load("./Datasets/okc.RData")
load("./Datasets/okc_other.RData")
load("./Datasets/okc_binary.RData")
load("./Datasets/okc_features.RData")

okc_train <-
  okc_train %>%
  full_join(okc_train_binary) %>%
  full_join(basic_features_train) %>%
  arrange(profile) %>%
  dplyr::select(-profile)

# create feature sets

basic_feat <- names(okc_test)
basic_feat <- basic_feat[!(basic_feat %in% c("Class", "profile", "essay_length", "where_state"))]

# keywords only

keyword_feat <- names(okc_train_binary)
keyword_feat <- keyword_feat[keyword_feat != "profile"]

text_feat <-
  c("n_urls", "n_hashtags", "n_mentions", "n_commas", "n_digits",
    "n_exclaims", "n_extraspaces", "n_lowers", "n_lowersp", "n_periods",
    "n_words", "n_puncts", "n_charsperword")

sent_feat <-
  c("sent_afinn", "sent_bing", "n_polite", "n_first_person", "n_first_personp",
    "n_second_person", "n_second_personp", "n_third_person", "n_prepositions")

# ------------------------------------------------------------------------------

many_stats <-
  function(data, lev = levels(data$obs), model = NULL) {
    c(
      twoClassSummary(data = data, lev = levels(data$obs), model),
      prSummary(data = data, lev = levels(data$obs), model),
      mnLogLoss(data = data, lev = levels(data$obs), model),
      defaultSummary(data = data, lev = levels(data$obs), model)
    )
  }

okc_ctrl <- trainControl(
  method = "cv",
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  returnData = FALSE,
  trim = TRUE,
  sampling = "down"
)

# ------------------------------------------------------------------------------
## First run a tensorflow model (since it cannot be run in parallel
## via foreach)

## Recipe for the keywords that normalizes the predictors at the end

keyword_norm <-
  recipe(Class ~ .,
         data = okc_train[, c("Class", basic_feat, keyword_feat)]) %>%
  step_YeoJohnson(all_numeric()) %>%
  step_other(where_town) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

okc_ctrl_rand <- okc_ctrl
okc_ctrl_rand$search <- "random"
set.seed(49)
mlp_keyword <-
  train(
    keyword_norm,
    data = okc_train,
    method = "mlpKerasDropout",
    metric = "ROC",
    tuneLength = 20,
    trControl = okc_ctrl_rand,
    verbose = 0,
    epochs = 10
  )

```

