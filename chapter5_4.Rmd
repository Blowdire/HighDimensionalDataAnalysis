---
title: "5.4 Supervised encoding"
output:
  pdf_document: default
  html_document: default
---

When the predictors have many possible values or when new levels appear after training, it’s recommended to encode using *supervised methods*, which can be implemented in different modalities.

**Likelihood encoding**

The first type consists of measuring the effect on the outcome by calculating a statistic for the categorical predictors and using this to represent the factor levels in the model. For example we can calculate the odds which is the ratio between two rates of occurrence (probabilities of complementary events), p and (1-p). A model like a logistic regression can then model the log-odds (*ln* of odds) as a function of the predictors. If a single categorical predictor is included in the model, then the log-odds can be calculated for each predictor value and can be used this as the encoding.

```{r path, echo=FALSE}
setwd("C:/Users/david/Desktop/Università/2 anno/High dimensional data analysis/Progetto")
```

```{r library, tidy=TRUE, results="hide", message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(embed)
library(ggplot2)
library(gridExtra)
library(pander) # for table layout
```

```{r odds, tidy=TRUE, include=TRUE, warning=FALSE}
load("okc.RData")
load("okc_binary.RData")

sample_towns <- c(
  "belvedere_tiburon", "berkeley", "martinez", "mountain_view", "san_leandro",
  "san_mateo", "south_san_francisco")

# get raw rates and log-odds

okc_props <- 
  okc_train %>%
  group_by(where_town) %>%
  summarise(
    rate = round(mean(Class == "stem"),3),
    n = length(Class),
    raw  = log(rate/(1-rate))
  ) %>%
  mutate(where_town = as.character(where_town)) %>% 
  rename(location = where_town)

```

The problems with this method begin when a categorical predictor has a single value and the log-odds should then be infinite (since it ranges from *-inf* to *+inf*). Generally this gets cut off at a large inaccurate value. To overcome this limit the so called *shrinkage methods* can be applied, where the overall log-odds is determined and, if the quality of the data within a factor level is poor (small sample size or, for numeric outcomes, a large variance within the data for that level), then its effect estimate can be biased towards an overall estimate that disregards the levels of the predictor. These methods can also move extreme estimates towards the middle of the distribution.

**Bayesian likelihood encoding**

There are different methods of shrinking, the most common is based on the prior distribution for the estimates, a theoretical distribution that represents the overall distribution of effects, like for example a normal distribution. Bayesian methods take the observed data and blend it with the prior distribution to come up with a posterior distribution that is a combination of the two. Also, when a new level appears after training, its effect can be estimated using the mean of the posterior distribution.

```{r shrink, tidy=TRUE, include=TRUE, warning=FALSE}
# fit a generalized linear model and return encoding

shrink_rec <- 
  recipe(Class ~ ., data = okc_train) %>%
  step_lencode_bayes(
    where_town,
    outcome = vars(Class),
    verbose = TRUE,
    options = list(
      chains = 5, 
      iter = 1000,
      cores = min(parallel::detectCores(), 5),
      seed = 18324)) %>% prep()

shrinken <- 
  tidy(shrink_rec, number = 1) %>%
  select(-terms, -id) %>%
  setNames(c("location", "shrunk")) %>% 
  full_join(okc_props, by="location") # merge with raw log-odds table

shrinken %>%
  filter(location %in% sample_towns) %>%
  relocate(shrunk, .after=raw) -> shrinken_sample # sample and move columns
shrinken_sample$location <- str_replace_all(shrinken_sample$location, "_", " ")

shrinken_sample %>% add_row(tail(shrinken, 1)) %>% pander(round=3)
```

The shrinking effect is shown below: plot a. compares the raw with the shrunken log-odds, while plot b. shows the magnitude of the estimates (mean between raw and shrinken odds) versus the difference between the two.
```{r plots, tidy=TRUE, warning=FALSE}
# plots where size of the points is Log of n. osservations

odds_rng <- extendrange(c(shrinken$raw, shrinken$shrunk), f = 0.01) # log-odds range

plot_a <-
  ggplot(shrinken) +
  geom_point(aes(x = raw, y = shrunk, size = log10(n)), alpha = 0.4, colour="red") + 
  scale_size(range = c(.1, 6)) +
  geom_abline(alpha = .4, lty = 2)  +
  xlim(odds_rng) +
  ylim(odds_rng) +
  xlab("Raw Log-Odds") +
  ylab("Shrunken Log-Odds") +
  theme(legend.position="None")

plot_b <- 
  ggplot(shrinken) +
  geom_point(aes(x = .5*(raw + shrunk), y = raw - shrunk, size = log10(n)), alpha = 0.4, colour="red") + 
  scale_size(range= c(.1, 6)) + 
  geom_hline(alpha = .4, lty = 2, yintercept = 0) + 
  xlab("Average Estimate") +
  ylab("Raw - Shrunken") +
  theme(legend.position="None")

ggpubr::ggarrange(plot_a, plot_b,
                  labels=c("a.","b."))
```

One issue with effect encoding is that it increases the possibility of overfitting since the estimated effects are taken from one model and put into another model (as variables) and if these two models are based on the same data it can lead to overfitting. Also, the use of summary statistics as predictors can drastically underestimate the variation in the data.

**Word embedding**

Another approach comes from the analysis of textual data: with *word embedding* the idea is to represent the categorical predictors as vectors and this cane be done with different algorithms in order to avoid sparsity. Once the number of new features are specified, the model takes the traditional indicators variables and randomly assigns them to one of the new features. The model then tries to optimize both the allocation of the indicators to features as well as the parameter coefficients for the features themselves. It’s common to use as a loss function the root mean squared error for numeric outcomes and cross-entropy for categorical outcomes.

A neural network structure can also be used, where the encoding happens in the hidden layers. This type of model can include a set of other unrelated predictors in order to adjust and better estimate the encoding in presence of other potentially important features.
```{r embed, tidy=TRUE, include=TRUE, warning=FALSE}
# merge training data with the keyword indicators

keywords <- names(okc_train_binary)[-1]

okc_embed <-
  okc_train %>% 
  select(Class, where_town, profile) %>%
  full_join(okc_train_binary, by = "profile")

# doubles instead of binary integers as tensorflow input

okc_embed[, keywords] <- apply(okc_embed[, keywords], 2, as.numeric)

# use the entity embedding

set.seed(355)
nnet_rec <- 
  recipe(Class ~ ., data = okc_embed) %>% 
  step_embed(
    where_town,
    outcome = vars(Class),
    num_terms = 3,
    hidden_units = 10,
    predictors = vars(!!!(keywords)),
    options = embed_control(
      loss = "binary_crossentropy",
      epochs = 30,
      validation_split = 0.2,
      verbose = 0
    )) %>% prep()

word_embed <- 
  tidy(nnet_rec, number = 1) %>%
  select(-terms, -id) %>%
  setNames(c(paste0("Feature", 1:3), "location"))

word_embed_sample <- 
  shrinken %>%
  inner_join(word_embed, by = "location") %>%
  select(location, rate, n, raw, shrunk, Feature1, Feature2, Feature3) %>% 
  filter(location %in% sample_towns)

word_embed_sample$location <- str_replace_all(word_embed_sample$location, "_", " ")

pander(word_embed_sample, round=3)

```

The figure below shows the relationship between the resulting features and the raw log-odds.

```{r fplot, tidy=TRUE, include=TRUE, warning=FALSE}
word_embed %>%
  full_join(okc_props, by = "location") %>%
  gather(feature, value, -rate, -raw, -n, -location) %>% 
  ggplot() +
  geom_point(aes(x = value, y = raw, size=log10(n)), alpha = 0.4, colour="red") + 
  facet_wrap(~feature, scale = "free_x") + 
  ylab("Raw Odds-Ratio") + 
  theme(legend.position = "top") + 
  xlab("Feature Value") + 
  scale_size(range = c(.1, 6)) +
  theme(legend.position="None")
```
